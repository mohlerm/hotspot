This chapter will provide the reader with an overview of the relevant parts of Java Hotspot. It explains the core concepts that are needed to understand the motivation and implementation of this thesis.

\section{Tiered Compilation}
\label{sec:tiered}
As mentioned in the introduction, Programming Language Virtual Machines like Java Hotspot feature a multi-tier system when compiling methods during execution. 
Java VM's typically use Java Bytecode as input, a platform independent intermediate code generated by a Java Compiler like \texttt{javac} \cite{javac}.
The Bytecode is meant to be interpreted by the virtual machine or further compiled into platform dependent machine code.
Hotspot includes one interpreter and two different just-in-time compilers with different profiling levels resulting in a total of 5 different tiers. Since in literature and the JVM source code use the \textit{tiers} are also called \textit{compilation levels} I use both as synonyms. 
\begin{figure}[ht]
  \begin{center}
    \centering
    \includegraphics{figures/hs_tiers.png}
    \caption{Overview of compilation tiers}
    \label{f:hs_tiers}
  \end{center}
\end{figure}

All methods start being executed by Tier 0, also called the interpreter.
The interpreter is template-based, meaning for each bytecode instruction it emits a predefined assembly code snippet.
During execution this code is also profiled. This means method information like execution counters, loop back-branches and additional statistics are counted. More importantly information about the program flow and state are gathered. These information contain for example which branches get taken or the final types of dynamically typed objects. Once one these counters exceed a predefined, constant threshold the method is considered \textit{hot} which usually results in a compilation at a higher tier.
\\\\
The standard behavior of Hotspot is to proceed with Level 3 (Tier 3). The method gets compiled with C1, also referred to as \textit{client} compiler, and continues gathering full profiles.
C1's goal is to provide a fast compilation with a low memory footprint.
The client compiler performs simple optimizations such as constant folding, null check elimination and method inlining.
It will also use the profiles gathered during interpretation. For example if certain branches were not taken during interpretation the C1 compiler might abstain from compiling these branches and provide a faster compilation.
\\\\
The levels 1 and 2 include the same optimization but offer no or less profiling information and are used in special cases. Code compiled at these levels is significantly faster than level 3 because it needs to execute none or little instructions creating and managing the profiles. Anyway, since the profiles generated by C1 are further used in C2, Hotspot is usually interested in creating full profiles and therefor use level 3.
There are however rare instances where a compilation of level 1 or level 2 is triggered. For example if enough profiles are available and a method can not be compiled by a higher tier, Hotspot might recompile the method with Tier 2 to get faster code until the higher tier compiler is available again. A compiler can become unavailable if its compilation queue exceeds a certain threshold.
\\\\
More information about C1 can be found in \cite{client_compiler_talk} and \cite{client_compiler}.
\\\\
Eventually, when further compile thresholds are exceeded, the JVM further compiles the method with C2, also known as \textit{server} compiler.
The server compiler makes use of the gathered profiles in Tier 0 and Tier 3 and produces highly optimized code. C2 includes far more and more complex optimizations like loop unrolling, common subexpression elimination and elimination of range and null checks. It performs optimistic method inlining, for example by converting some virtual calls to static calls. It relies heavily on the profiling information and richer profiles allow the compiler to use more and better optimizations.
While the code quality of C2 is a lot better than C1 this comes at the cost of compile time. Since a C2 compilation includes 
A more detailed look at the server compiler can be found in \cite{server_compiler}.
Figure \ref{f:hs_tiers} gives a short overview as well as showing the standard transition.
\\\\
The naming scheme \textit{client/server} comes from back in the days were tiered compilation was not available and one had to choose the compiler via a Hotspot command line flag. The \textit{client} compiler was meant to be used for interactive client programs with graphical user interfaces where response time is more important than peak performance. For long running server applications, the highly optimized but slower \textit{server} compiler was used. 
\\\\
Tiered compilation was introduced to improve start-up performance of the JVM.
Starting with the interpreter means that there is zero wait time until the method is executed since one does not need to wait until a compilation is finished. Also, consider that there are always parts of the code that get executes only once, where the compilation overhead would exceed the performance gain. C1 allows the JVM to have more optimized of the code available early which then can be used to create a richer profile to be used when compiling with C2. Ideally this profile already contains most of the program flow and the assumptions made by C2 hold. If that is not the case the JVM might need to go back, gather more profiles and compile the method again. This is further described in the Section \ref{s:deoptimizations} \textit{Deoptimizations}. In this case, being able to do quick compilations with C1 decreases the amount of C2 recompilations which are even more costly.

\section{Deoptimizations}
\label{s:deoptimizations}
Ideally we compile a method with as much profiling information as possible.
For example, since the profiling information are usually gathered in levels 0 and 3 it can happen that a method compiled by C2 wants to execute a branch it never used before.
In this case the information about this branch are not available in the profile and therefore have not been compiled into the C2-compiled code.
This is done to allow further, very optimistic optimization and to keep the compiled code smaller. So instead, the compiler places an uncommon trap at unused branches or unloaded classes which will get triggered in case they actually get used at a later time in execution.
\\\\
The JVM then stops execution of that method and returns the control back to the interpreter. This process is called \textit{deoptimization} and considered very expensive. The previous interpreter state has to be restored and the method will be executed using the slow interpreter. Eventually the method might get recompiled with the newly gained information.

\section{Compile Thresholds}
\label{s:compilethresholds}
The transitions between the compilation levels (see Fig. \ref{f:hs_tiers}) are chosen based on predefined constants called \textit{compile thresholds}. When running an instance of the JVM one can specify them manually or use the ones provided. A list of thresholds and their default values relevant to this thesis are given in Appendix \ref{a:compilethresholds}.
The standard transitions from Level 0 to 3 and 3 to 4 happen when the following predicate returns true:
\begin{align*}
& i > TierXInvocationThreshold \ * \ s \\
 || \ (&i > TierXMinInvocationThreshold \ * \ s \ \&\& \ i \ + \ b \ > \ TierXCompileThreshold \ * \ s) 
\end{align*}
where $X$ is the next compile level (3 or 4), $i$ the number of method invocations, $b$ the number of backedges and $s$ a scaling coefficient (default = 1).
The thresholds are relative and individual for interpreter and compiler.
\\\\
On-stack replacement uses a simpler predicate:
$$b > TierXBackEdgeThreshold * s$$

Please note that there are further conditions influencing the compilation like the load on the compiler which will not be discussed.

\section{On-Stack Replacement}
\label{s:onstackreplacement}
Since the JVM does not only count method invocations but also loop back branches (see also Section \ref{s:compilethresholds}) it can happen that a method gets compiled while it is still running and the compiled method is ready before the method has finished.
Instead of waiting for the next method invocation Hotspot can replace the method directly on the program stack. This process is called \textit{on-stack replacement} and usually shortened to OSR. The Figure \ref{f:osr} presented in a talk by T. Rodriguez and K. Russel \cite{client_compiler_talk} gives a graphical representation.
The benefits of OSR will become more obvious when looking at the first example in Chapter \ref{c:motivation}.
\begin{figure}[ht]
  \begin{center}
    \centering
    \includegraphics[width=0.6\textwidth]{figures/osr.png}
    \caption{Graphical schema of OSR}
    \label{f:osr}
  \end{center}
\end{figure}
